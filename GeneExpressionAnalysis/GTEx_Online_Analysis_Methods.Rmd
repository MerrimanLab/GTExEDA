---
title: "GTEx_Online_Analysis_Methods"
author: "Nick Burns"
date: "13 June 2016"
output: html_document
---

# Online analysis methods for GTEx Gene Expression data  

Previously, we have explored some very simple visualisation of the GTEx gene expression data which include questions like: "how does the expression of gene, G1, compare across tissues?". See GTExVisualisation.Rmd. For many users, simple questions and simple visualisations will be sufficient but, we might also be interested in more complex questions such as: "which other genes are similar to G1?" or, "which other tissues share common expression patterns to T1?".  

Unfortunately, multivariate methods typically rely on the covariance matrix which is intractable at this data size (raw file ~ 4.7 GB). We culd pre-filter the data based on specific domain knowledge, but this would restrict our ability to detect potentially novel relationships. Instead, we will explore online (or incremental) machine learning algorithms to solve this problem. 

There is just one problem... I can't find any R packages which provide online implementations of kmeans or principal components analysis etc. Python has some (see http://scikit-learn.org/stable/modules/scaling_strategies.html) - but I can't get python to even read the 4.7 GB file. Fail. 

```{r}
setwd("~/Documents/GitHub/GTExEDA")
library(data.table)

dataDir <- "/mnt/DataDrive/gEXPR_eQTL_Datasets/GTEXData/%s"
gtexFile <- sprintf(dataDir, "GeneExpression/All_Tissue_Site_Details_Analysis.combined.rpkm.gct")

gtex <- fread(gtexFile)
gtex[1:5, 1:7, with = FALSE]
```

I have tried a whole lot of different libraries with no success. RMOA & stream I couldn't figure out how to work - dumb. Birdh is redundant and no longer available. The two issues I am having are:

  - in python I am not familiar enough with how to read in a file this large. If I could and i could chunk it out, then I could use MiniBatchKmeans.  
  - using data.table I can easily read this in in R - so perhaps I simply implement my own mini batch kmeans, it can't be that hard!  
  
## Implementing mini-batch kmeans  

Will use a training sample of 5000 genes to estimate the number of clusters...
```{r}
idx_train <- sample(nrow(gtex), 5000)
chunk1 <- gtex[idx_train, -c(1, 2), with = FALSE]

#pca <- chunk1[, prcomp(.SD, scale = TRUE)]
results <- lapply(seq(3, 30, 3), function (k) {
    print(k)
    ks <- chunk1[, kmeans(scale(.SD), k)]
    
    return (ks$tot.withinss)
})
plot(unlist(results)/1000000)
```

Wow, there are waaaay fewer gene clusters than I would have thought. Although this is euclidean distance...

Am now going to overfit the number of clusters, and will recluster the centroids later

```{r}
idx_chunks <- sample(nrow(gtex)) %% 10   # 10 chunks
head(idx_chunks, 20)
table(idx_chunks)
```

Estimate the scaling factors & principcal components (this is for visualisation only, so no need to stress to much) from the first chunk, save these values.

```{r}
xs <- gtex[idx_chunks == 0, -c(1, 2), with = FALSE][, scale(.SD, center = TRUE, scale = TRUE)]
xs.centers <- attr(xs, "scaled:center")
xs.scale <- attr(xs, "scaled:scale")

xs.pca <- gtex[idx_chunks == 0, -c(1, 2), with = FALSE][, prcomp(.SD, scale = TRUE)]
summary(xs.pca)$importance[, 1:5]
```

Calculate initial centroids using the first chunk, and then incrementally update from then on...