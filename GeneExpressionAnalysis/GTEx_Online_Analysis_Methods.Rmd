---
title: "GTEx_Online_Analysis_Methods"
author: "Nick Burns"
date: "13 June 2016"
output: html_document
---

# Online analysis methods for GTEx Gene Expression data  

Previously, we have explored some very simple visualisation of the GTEx gene expression data which include questions like: "how does the expression of gene, G1, compare across tissues?". See GTExVisualisation.Rmd. For many users, simple questions and simple visualisations will be sufficient but, we might also be interested in more complex questions such as: "which other genes are similar to G1?" or, "which other tissues share common expression patterns to T1?".  

Unfortunately, multivariate methods typically rely on the covariance matrix which is intractable at this data size (raw file ~ 4.7 GB). We culd pre-filter the data based on specific domain knowledge, but this would restrict our ability to detect potentially novel relationships. Instead, we will explore online (or incremental) machine learning algorithms to solve this problem. 

One solution is to use alternate centroid-centric methods of clustering, such as kmeans. The computational cost of kmeans is asymptotically O(kN), where k << N. This is clearly favourable over the O(N^2) costs of calculating a covariance matrix. Even so, the cost of kmeans grows quickly when N is large and as k increases thus, limiting the degree of k which may be fit. Our goals are to explore a) the solutions to mini-batch kmeans, b) the averaged solutions across all batches and c) whether the resulting centroids can be interpreted (perhaps through heirarchical clustering). In short, we hope that a mini-batch kmeans approach might be used to summarise a large dataset, and hope to show that this summary is biologically meaningful.

## Test data  

For this, we are going to work with the Creighton cancer dataset. Specifically, we will focus on the 799 "obesity signature genes". Whilst this is not a giant dataset, it is sufficiently large to explore and interrogate our goals.

```{r}
setwd("~/Documents/GitHub/GTExEDA/")
library(data.table)
library(ggplot2)

# read in trait and expression data
traits <- fread("~/Google Drive/Projects/ICAPlaypen/GSE24185_clinical.csv")
expression <- read.table("~/Google Drive/Projects/ICAPlaypen/GSE24185_series_matrix.txt", sep = "", comment.char = "!", header = TRUE, row.names = 1)
gene_names <- rownames(expression)
expression <- data.table(expression)

# filter to obesity-related genes
obesity_genes <- readLines('~/Google Drive/Projects/ICAPlaypen/crobsgenes.txt')
probes <- as.vector(na.omit(match(obesity_genes, gene_names)))
expression <- as.matrix(expression[probes])   # note probes is a vector of row indices.

# standardise (z-scores of gene expression)
expression <- apply(expression, 1, function (x) (x - mean(x)) / sd(x))
expression[expression < -3] <- 3
expression[expression > 3] <- 3
expression <- t(expression)

# a quick look
rownames(expression) <- gene_names[probes]
dim(expression)
head(expression[, 1:10])
```

This dataset is small enough that we can apply basic clustering, for example a heatmap:

```{r}
orig.tree1 <- hclust(as.dist(1 - cor(t(expression), method = "spearman")))
orig.tree2 <- hclust(dist(scale(expression)))

par(mfrow = c(2, 1))
plot(orig.tree1, labels = FALSE, xlab = NA, main = NULL)
plot(orig.tree2, labels = FALSE, xlab = NA, main = NULL)
par(mfrow = c(1, 1))
```

The dendrogram of the gene clusters is quite nice. Clearly, there is one clear cluster of genes on the left, and then at least 2 clusters on the right of the tree. Let's begin with 3 clusters intially.

```{r}
targets <- cutree(orig.tree2, 4)
table(targets)
```

## Mini-batch kmeans  

In a true mini-batch implementation, the cluster centers should evolve between batches. Here, we will simply create new centers and then plot them all at the end. Plan:  

  - divide the dataset into 6 batches  
  - perform kmeans, over fitting initially  
  - plot the centroids from each run (require principal components to do this)  
  
```{r}
batches <- sample(nrow(expression)) %% 9
table(batches)

pca <- prcomp(expression, scale = FALSE, center = FALSE)

par(mfrow = c(3, 3))

centroids <- NULL
for (l in unique(batches)) {
    tmp <- expression[batches == l, ]
    ks <- kmeans(tmp, 15)
    lcl_centroids <- ks$centers %*% pca$rotation[, 1:10]
    
    lcl_result <- as.data.frame(ks$centers)
    lcl_result$batch_no <- l
    if (is.null(centroids)) {
        centroids <- lcl_result
    } else {
        centroids <- rbind(centroids, lcl_result)
    }
    
    plot(pca$x[, 1:2], col = "grey", main = l)
    points(lcl_centroids[, 1:2], col = "red")
}
par(mfrow = c(1, 1))
```

There is a lot of consistency in the centroid positions which is good to see. We can combine these all into one plot:

```{r}
plot(pca$x[, 1:2], col = "grey")
points(as.matrix(centroids[, -ncol(centroids)]) %*% pca$rotation[, 1:2], col = "blue")
```

I quite like this, in that these centroids combined provide quite a good summary of the original data.

```{r}
summary_clusters <- kmeans(centroids[, -ncol(centroids)], 4)
points(summary_clusters$centers %*% pca$rotation[, 1:2], col = "red", pch = 19)
text(summary_clusters$centers %*% pca$rotation[, 1:2] * 1.02, col = "red", labels = 1:4)
```

Visually, this appears to be a reasonably good summarisation of the original dataset. We overfit each batch, and then overfit the aggregate of all the batches to give us 20 summary values. There are definitely disadvantages to this, in that the resulting summary_clusters are quite variable depending on the particular kmeans run. However, we should be able to use these in a "nearest neighbour" type of approach.  

Our 90 centroids (6 batches x 15 centers) have been reclustered into 10 groups, meaning that we can now assign cluster membership to each centroid and train a nearest neighbour type of model:

```{r}
centroids$cluster <- summary_clusters$cluster
centroids$batch_no <- NULL
head(centroids[, c(1:5, ncol(centroids))])
```

We can now train a nearest neighbour model, and use this to fit the original expression data:

```{r}
library(kknn)

model <- train.kknn(factor(cluster) ~ ., data = centroids, ks = 10)
table(centroids$cluster, fitted.values(model)[[1]])
```

This has done a spectacular job on the centroids. But how does it go on the expression data?

```{r}
predicted_clusters <- predict(model, as.data.frame(expression))

plot(orig.tree2, labels = predicted_clusters)
table(targets, predicted_clusters)
```

This isn't perfect - by a long shot. But the most important thing is that it has accurately identified the left-most cluster, those outlying genes that we were interested in. There is a lot of admixture in the larger group of genes, but at least they are well separated from the smaller group.

We can see this visually in the following plot:

```{r}
plot(pca$x[, 1:2], col = "grey")
points(as.matrix(centroids[, -ncol(centroids)]) %*% pca$rotation[, 1:2], col = "blue")

points(summary_clusters$centers %*% pca$rotation[, 1:2], col = "red", pch = 19)
text(summary_clusters$centers %*% pca$rotation[, 1:2] * 1.02, col = "red", labels = 1:4)
```

Cluster 3 is well separated from the others. And, I just boosted the number of nearest neighbours from 10 to 30 and it has done an even better job. There is some potential in this perhaps.

## Final thoughts  

What we have shown here, is that an averaged mini-batch approach to kmeans can do quite well at clustering this dataset. It isn't perfect by a long shot - but it does achieve the most important factor, which is it has separated the two obvious groups of genes - and this is what matters most here. I think that the uncertainty in the larger group is a good thing, and it reflects the fact that there is very little indpendent signal in this larger group.

So, what have we done?  

  1. over-clustered batches of the input data to construct a "summary" of each batch  
  2. clustered these summaries to identify clusters  
  3. showed that it picked up the strongest clusters in the data  
  
```{r}
par(mfrow = c(2, 1))
plot(orig.tree1, labels = FALSE, xlab = NA, main = NA)
plot(hclust(as.dist(1 - cor(t(centroids[, -ncol(centroids)]), method = "spearman"))), xlab = NA, main = NA, labels = FALSE)
par(mfrow=c(1,1))
```
  
Why do we want to do this?  Because the data is too large to cluster in one go, so we cluster it in small batches, produce a summary and then predict cluster membership based on this summary. Based on the two plots above, this is analagous to pruning the dendrogram.

We have shown that there are two clear groups - I need to simulate this to show that this also works when there are more than 2 clear groups, i.e. it should be able to pick up all clear clusters.